## Week 46 – Convolutional networks and regularization

- **Reading:** Chapter 21 sections 21.3–21.4 on **CNNs**, pooling, and regularization techniques like weight decay, dropout, and batch normalization.[aima.berkeley+1](https://aima.cs.berkeley.edu/contents.html)​

- **Coding / exercises:**

   - Build and train a small **CNN** on MNIST or Fashion‑MNIST, comparing it to your earlier MLP on accuracy and robustness.[aikosh.indiaai+1](https://aikosh.indiaai.gov.in/static/Deep+Learning+Ian+Goodfellow.pdf)​

   - Experiment with regularization (L2, dropout) and document how they affect training vs. validation loss and overfitting.[aikosh.indiaai+1](https://aikosh.indiaai.gov.in/static/Deep+Learning+Ian+Goodfellow.pdf)​

- **Reflection:** Write a short note on how convolution and pooling exploit spatial structure, and how regularization balances model capacity and generalization.[aima.berkeley+1](https://aima.cs.berkeley.edu/contents.html)​
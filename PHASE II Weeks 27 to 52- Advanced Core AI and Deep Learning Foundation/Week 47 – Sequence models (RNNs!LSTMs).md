## Week 47 – Sequence models (RNNs/LSTMs)

- **Reading:** Chapter 21 sections 21.5–21.6 on **recurrent neural networks**, sequence modeling, and LSTMs.[aima.berkeley+1](https://aima.cs.berkeley.edu/)​

- **Coding / exercises:**

   - Implement or use a framework to train a small RNN or LSTM for a **sequence task**, such as character‑level language modeling or simple time‑series prediction.[aikosh.indiaai+1](https://aikosh.indiaai.gov.in/static/Deep+Learning+Ian+Goodfellow.pdf)​

   - Visualize training/validation loss over time and generate a sample sequence (e.g., text characters) to build intuition about what the model learns.[aikosh.indiaai+1](https://aikosh.indiaai.gov.in/static/Deep+Learning+Ian+Goodfellow.pdf)​

- **Reflection:** Explain how sequence models relate to HMMs conceptually, noting differences in how they represent and learn temporal dependencies.[modanesh.github+1](https://modanesh.github.io/assets/Summary%20Artificial%20Intelligence%20A%20Modern%20Approach.pdf)​
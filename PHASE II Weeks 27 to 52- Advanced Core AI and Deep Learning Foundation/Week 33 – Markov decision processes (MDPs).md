## Week 33 – Markov decision processes (MDPs)

- **Reading:** Chapter 17 “Making Complex Decisions” sections on MDP definition, states/actions/rewards, and utility of states; read up to before algorithms.[aima.berkeley+1](https://aima.cs.berkeley.edu/)​

- **Coding / exercises:**

   - Implement a small **gridworld MDP** with states, actions, transition probabilities, and rewards as described in the chapter.[aima.berkeley](https://aima.cs.berkeley.edu/contents.html)​

   - Write code to evaluate a fixed policy by simulating rollouts and estimating long‑run cumulative reward under discount factor γ\\gammaγ.[aima.berkeley](https://aima.cs.berkeley.edu/contents.html)​

- **Reflection:** In ½–1 page, explain how MDPs generalize decision networks to multi‑step problems, and how they differ from the planning problems you solved earlier (uncertainty and rewards vs. deterministic goals).[modanesh.github+1](https://modanesh.github.io/assets/Summary%20Artificial%20Intelligence%20A%20Modern%20Approach.pdf)​
## Week 45 – Feedforward networks and computation graphs

- **Reading (1–2 sessions):** Chapter 21 “Deep Learning” sections 21.1–21.2 on simple feedforward networks, gradients, and computation graphs (input encoding, loss, hidden layers).[aima.berkeley+1](https://aima.cs.berkeley.edu/)​

- **Coding / exercises (1–2 sessions):**

   - Implement a small **MLP** (one hidden layer) from scratch using NumPy or a similar library, including forward and backward passes for a simple classification task.[aikosh.indiaai+1](https://aikosh.indiaai.gov.in/static/Deep+Learning+Ian+Goodfellow.pdf)​

   - Then replicate the same model using a deep‑learning framework (PyTorch or TensorFlow/Keras) to compare code clarity and speed.[scribd+1](https://www.scribd.com/document/905797212/Artificial-Intelligence-A-Modern-Approach-4th-Edition-Stuart-Russell-eBook-PDF-Full-Access)​

- **Reflection:** In ½–1 page, explain how computation graphs and backpropagation relate to the gradient descent you used for logistic regression, noting what “deep” adds beyond shallow linear models.[aikosh.indiaai+1](https://aikosh.indiaai.gov.in/static/Deep+Learning+Ian+Goodfellow.pdf)​
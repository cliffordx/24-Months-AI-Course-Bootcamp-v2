## Week 34 – Value iteration and policy iteration

- **Reading:** Chapter 17 sections on **value iteration** and **policy iteration**, including Bellman equations and convergence discussions.[aima.berkeley+1](https://aima.cs.berkeley.edu/)​

- **Coding / exercises:**

   - Implement value iteration for your gridworld MDP and track value function updates across iterations until convergence.[aima.berkeley](https://aima.cs.berkeley.edu/contents.html)​

   - Implement policy iteration and compare convergence speed and resulting policies to value iteration on the same environment.[aima.berkeley](https://aima.cs.berkeley.edu/contents.html)​

- **Reflection:** Write about the intuition for Bellman backups, how discounting affects optimal policies, and when you might choose policy iteration vs. value iteration.[modanesh.github+1](https://modanesh.github.io/assets/Summary%20Artificial%20Intelligence%20A%20Modern%20Approach.pdf)​
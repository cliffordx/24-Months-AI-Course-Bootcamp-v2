## Week 44 – Learning HMMs and chapter review

- **Reading:** Chapter 20 sections on **learning with hidden variables**, particularly EM for HMMs and general discussion of incomplete data.[aima.berkeley+1](https://aima.cs.berkeley.edu/contents.html)​

- **Coding / exercises:**

   - Implement (or adapt) **Baum–Welch** EM learning for a small HMM, training it on observation sequences and comparing learned parameters with the ones you used earlier.[stonybrook+1](https://www3.cs.stonybrook.edu/\~sael/teaching/cse537/Slides/chapter20a.pdf)​

   - Run experiments showing how more data improves parameter estimates or log‑likelihood on a validation set.[modanesh.github+1](https://modanesh.github.io/assets/Summary%20Artificial%20Intelligence%20A%20Modern%20Approach.pdf)​

- **Reflection:** Produce a one‑page “Learning Probabilistic Models” summary covering ML vs. MAP vs. full Bayesian, EM, and learning BNs/HMMs, and note where these ideas connect to modern deep generative models.[pageplace+1](https://api.pageplace.de/preview/DT0400.9781292401171_A41586057/preview-9781292401171_A41586057.pdf)​
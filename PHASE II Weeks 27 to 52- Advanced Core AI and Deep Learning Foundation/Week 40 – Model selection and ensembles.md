## Week 40 – Model selection and ensembles

- **Reading:** Chapter 19 sections on **model selection**, regularization, and ensemble methods (bagging, boosting) at a conceptual level.[aima.berkeley+1](https://aima.cs.berkeley.edu/)​

- **Coding / exercises:**

   - Implement L2‑regularized logistic regression and at least one simple ensemble (e.g., bagging of small trees) using your existing code or a library.[studylib+1](https://studylib.net/doc/25797353/table-of-contents-for-ai--a-modern-approach-3rd-edition)​

   - Use cross‑validation to choose hyperparameters (regularization strength, number of trees) and document performance changes.[moodle2425.up+1](https://moodle2425.up.pt/pluginfile.php/176152/mod_resource/content/8/metrics.pdf)​

- **Reflection:** Summarize how regularization and ensembles reduce overfitting differently, and what costs they introduce (computation, interpretability).[modanesh.github+1](https://modanesh.github.io/assets/Summary%20Artificial%20Intelligence%20A%20Modern%20Approach.pdf)​
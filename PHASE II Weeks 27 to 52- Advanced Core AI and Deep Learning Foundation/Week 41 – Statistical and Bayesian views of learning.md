## Week 41 – Statistical and Bayesian views of learning

- **Reading (1–2 sessions):** Chapter 20 “Learning Probabilistic Models” sections 20.1–20.2 (statistical learning, Bayesian learning, maximum likelihood for complete data).[aima.berkeley+1](https://aima.cs.berkeley.edu/contents.html)​

- **Coding / exercises (1–2 sessions):**

   - Implement **maximum‑likelihood parameter learning** for a simple discrete distribution (e.g., coin bias, categorical outcomes) and verify results with closed‑form formulas.[stonybrook+1](https://www3.cs.stonybrook.edu/\~sael/teaching/cse537/Slides/chapter20a.pdf)​

   - Revisit naive Bayes from your earlier work and re‑derive its parameter estimates via ML, logging counts and smoothed estimates.[stonybrook+1](https://www3.cs.stonybrook.edu/\~sael/teaching/cse537/Slides/chapter20a.pdf)​

- **Reflection:** In a short note, contrast the ML and **Bayesian** views of learning and explain when small datasets make Bayesian priors particularly valuable.[modanesh.github+1](https://modanesh.github.io/assets/Summary%20Artificial%20Intelligence%20A%20Modern%20Approach.pdf)​
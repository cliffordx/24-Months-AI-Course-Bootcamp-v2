## Week 42 – Learning with complete data: Gaussians and mixtures

- **Reading:** Chapter 20 sections on learning with complete data, including parametric continuous models (Gaussians) and basic mixture models.[studylib+1](https://studylib.net/doc/25797353/table-of-contents-for-ai--a-modern-approach-3rd-edition)​

- **Coding / exercises:**

   - Implement ML estimation for a univariate Gaussian (mean/variance) and a simple **mixture of Gaussians** using expectation–maximization (EM) on 1‑D or 2‑D data.[stonybrook+1](https://www3.cs.stonybrook.edu/\~sael/teaching/cse537/Slides/chapter20a.pdf)​

   - Visualize the learned components and responsibilities to build EM intuition.[modanesh.github+1](https://modanesh.github.io/assets/Summary%20Artificial%20Intelligence%20A%20Modern%20Approach.pdf)​

- **Reflection:** Describe how EM alternates between “soft assignment” and parameter updates, and what assumptions allow it to converge to a local optimum.[studylib+1](https://studylib.net/doc/25797353/table-of-contents-for-ai--a-modern-approach-3rd-edition)​
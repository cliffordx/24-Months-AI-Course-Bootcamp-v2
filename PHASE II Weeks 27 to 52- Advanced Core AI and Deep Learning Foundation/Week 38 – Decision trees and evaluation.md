## Week 38 – Decision trees and evaluation

- **Reading:** Chapter 19 sections on **decision‑tree learning**, entropy, information gain, overfitting, and evaluation (cross‑validation, learning curves).[aima.berkeley+1](https://aima.cs.berkeley.edu/)​

- **Coding / exercises:**

   - Implement a decision‑tree learner for Boolean or categorical features, using information gain as the splitting criterion and majority class at leaves.[aimacode.github+1](https://aimacode.github.io/aima-exercises/concept-learning-exercises/)​

   - Add functionality for depth limits and pruning or stopping rules, then compare train vs. test accuracy and plot a simple learning curve.[studylib+1](https://studylib.net/doc/25797353/table-of-contents-for-ai--a-modern-approach-3rd-edition)​

- **Reflection:** Explain in your notes why overfitting occurs, how pruning and validation combat it, and how this relates to the **bias–variance** trade‑off discussed in the chapter.[modanesh.github+1](https://modanesh.github.io/assets/Summary%20Artificial%20Intelligence%20A%20Modern%20Approach.pdf)​
## Week 27 – Viterbi and most likely sequences

- **Reading:** Chapter 14 section on **Viterbi algorithm** and most likely explanation, plus any example HMM problems.[aima.berkeley+1](https://aima.cs.berkeley.edu/)​

- **Coding / exercises:**

   - Implement **Viterbi** for your HMM and use it to find the most likely hidden state sequence given an observation sequence.[aima.berkeley](https://aima.cs.berkeley.edu/contents.html)​

   - Compare the Viterbi path to the per‑time‑step MAP states from filtering/smoothing; note where they differ.[aima.berkeley](https://aima.cs.berkeley.edu/contents.html)​

- **Reflection:** Write about the conceptual difference between “most probable sequence” (Viterbi) and “most probable state at each time,” and why both are useful in applications like POS tagging or tracking.[modanesh.github+1](https://modanesh.github.io/assets/Summary%20Artificial%20Intelligence%20A%20Modern%20Approach.pdf)​
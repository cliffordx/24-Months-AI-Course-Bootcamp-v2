## Week 81 – Evaluation framework and experiments

- **Reading:** Re‑read evaluation sections scattered through Chapters 1, 19–22, and 23 (performance measures, learning evaluation, RL metrics, NLP metrics).aima.berkeley+1​

- **Coding / design:**

   - Define **quantitative metrics**: task success rate, plan optimality gap, regret, accuracy/F1 for classifiers, RL cumulative reward, latency, etc., depending on your capstone.aimacode.github+1​

   - Implement an experiment harness that runs many episodes or tasks end‑to‑end, logs metrics, and supports replots and ablations (e.g., disabling planner, disabling LLM, disabling RL).stonybrook+1​

- **Reflection:** Write a short plan for the core experiments you will run (baseline vs. full system vs. ablated variants) and what hypotheses they test about your design.modanesh.github+1​